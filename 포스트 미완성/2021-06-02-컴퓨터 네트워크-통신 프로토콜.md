---
title: "활성함수 (ReLU, sigmoid, tanh, leakyReLU)"
excerpt: "딥러닝에서 사용되는 활성함수의 설명과 장단점"
categories:
    - 딥러닝
    - 공통
tags:
    - 활성함수
    - ReLU
    - sigmoid
    - tanh
    - leakyReLU
toc: true
toc_sticky: true
use_math: true
---

## 1. Sigmoid
$$y = \frac{1}{1+exp(-x)}$$


### 1. 시그모이드 함수 특징
1. 비선형 함수.
    1. 비선형 함수는 문자 그대로 '선형이 아닌' 함수 입니다. 즉, 직선 1개로 그릴 수 없는 함수를 그릴 수 없는 함수를 말합니다.<br/>
    2. 신경망에서는 활성함수로 비선형함수를 사용해야 합니다. 예를 들어 h(x) = cx를 활성함수로 사용한 3층 네트워크를 떠올려보세요. 이를 식으로 나타내면 $y(x) = h(h(h(x)))$가 됩니다. 이 계산은 $y(x) = ax$와 똑같은 식입니다. a = c<sup>3</sup>라고만 쓰면 되는 것이죠. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로는 반드시 비선형 함수를 사용해야 합니다.<br/>
2. Vanishing Gradient 문제 원인이 되는 함수 입니다. sigmoid는 0 ~ 1사이의 값을 가지기 때문에, 딥러닝의 깊이가 깊어질 수록 역전파의 갱신값은 점점 작아져서 소실되게 됩니다. 또한, sigmoid 자체의 계산량이 크기 때문에 학습시간도 오래걸린다는 단점이 있습니다. 보통은 이 문제를 해소하기 위해 ReLU함수를 대신 사용합니다.

### 2. 설명
1. "칵스"라는 사람이 sigmoid함수를 만들었습니다.

2. sigmoid 함수는 기본적으로 odds비에서 로그를 취해준 logits에서 파생된 함수입니다. odds비는 사건의 성공확률 / 실패확률을 나타내는 수식이며, 식은 y = "성공확률" 일 때, $ \frac{y}{1-y} $
로 나타냅니다.
3. odds비를 사용하지 않고, 로그를 취해 sigmoid함수를 취하는 이유는 활성함수의 출력값이 0 ~ 1사이가 되어야 하기 때문입니다. odds비는<br/> 
$$ odds = \frac{y}{1-y}$$ 
<br/>
$$ if:y = 1 일 때, answer = \infty $$
<br/>
$$ elif : y = 0 일 때, answer = 0$$
<br/>
이 되어버립니다.
4. 출력값이 0 ~ 1사이가 되어야 하는 이유는 다음과 같습니다. odds는 사건확률 / 사건이 일어나지 않을 확률 이기 때문에 1개의 사건이 일어나지 않았을 때의 성공의 기대값이라고 생각이 가능합니다. 그렇기 때문에, odds가 4 즉 4/1일 때, 4번의 사건이 일어나면 1번은 반대에 해당하는 즉, 실패사건이 일어나게 됩니다. 만약, y가 1에 가까우면, 무한번 시도해야 1번의 실패가 일어나는 것이어야 하고, y가 0에 수렴하면, -무한번(존재하지 않는 수)를 시도해야 합니다.
5. 이러한 단점을 보완하기 위해 자연로그를 취하게 됩니다. 자연로그 0은 무한대의 값을 가지며, e는 무리수이면서 초월수이기에 자기자신조차 나눠떨어지지 않으며 odds에 자연로그를 취해주기 적합합니다.
6. 우리는 평소, y의 값을 도출하기 위해 y = ax + b와 같은 선형식을 써왔습니다. a를 weight의 첫글자인 w로 대신하고, odds에 log를 취해준 값과 식을 합치게 되면, 다음과 같은 수식이 탄생합니다. <br/>
$$ln(\frac{y}{1-y}) = wx + b$$

### 3. 증명
1. 위에서 설명한 $ ln(\frac{y}{1-y}) = wx + b$을 전개해서, sigmoid함수로 만들어 보겠습니다.<br/><br/>
$$
ln(\frac{y}{1-y}) = wx + b
$$
<br/><br/>
$$
 e^{wx+b} = \frac{y}{1-y}
$$
<br/><br/>
$$
  \frac{1}{e^{wx+b}} = \frac{1-y}{y} = \frac{1}{y} - 1
$$
<br/><br/>
$$
 1 + \frac{1}{e^{wx+b}} = \frac{1}{y}
$$
<br/><br/>
$$
\frac{e^{wx+b}}{e^{wx+b}} +\frac{1}{e^{wx+b}} = \frac{1}{y}
$$
<br/><br/>
$$
\frac{1 + e^{wx+b}}{e^{wx+b}} = \frac{1}{y}
$$
<br/><br/>
$$
\frac{e^{wx+b}}{1 + e^{wx+b}} = y 
$$
<br/><br/>
$$
\frac{\frac{1}{e^{wx+b}}}{\frac{1}{e^{wx+b}}} * \frac{e^{wx+b}}{1 + e^{wx+b}} = y 
$$
<br/><br/>
$$
\frac{1}{\frac{1 + e^{wx+b}}{e^{wx+b}}} = y
$$
<br/><br/>
$$
\frac{1}{\frac{1}{e^{wx+b} + 1}} = y
$$
<br/><br/>
$$
\frac{1}{1+e^{-wx+b}} = y
$$
<br/><br/>
2. 딥러닝에서는 학습을 위해 Back-propagation을 사용합니다. 이 때 sigmoid를 미분하는 과정이 필요하게 되므로, 이번 증명에서 미분까지 진행하도록 하겠습니다. y = wx + b를 편의상 g(x)로 치환하겠습니다.<br/>
sigmoid또한, S(x)로 부분적으로 사용하겠습니다.<br/><br/>
$$
\partial s(g(x)) = \partial (\frac{1}{1+e^{-g(x)}})
$$
<br/><br/>
$$
=\partial (1+e^{-g(x)})^{-1}
$$
<br/><br/>
$$
=-(1+e^{-g(x)})^{-2} * \partial (1+e^{-g(x)})
$$
<br/><br/>
$$
=-(1+e^{-g(x)})^{-2} * e^{-g(x)}-1
$$
<br/><br/>
$$
=\frac{e^{-g(x)}}{(1+e^{-g(x)})^2}
$$
<br/><br/>
$$
=\frac{1+e^{-g(x)}-1}{(1+e^{-g(x)})^2}
$$
<br/><br/>
$$
\frac{-1}{(1+e^{-g(x)})}
$$
<br/><br/>
$$
=\frac{1}{(1+e^{-g(x)})} - \frac{1}{(1+e^{-g(x)})^2}
$$
<br/><br/>
$$
= \frac{1}{1+e^{-g(x)}} * (1 - \frac{1}{1+e^{-g(x)}})
$$
<br/><br/>
$$
=S(g(x)) * (1 - S(g(x)))
$$
<br/><br/>
---
<br/>
<br/>

## 2. ReLU 함수
$$
ReLU(x) = max(0,x) if(x <= 0) = 0, else(x > 0) = x
$$
<br/>
<br/>
### 1. ReLU 함수 특징<br/>
1. Sigmoid 함수와 비교해보면 계산량이 적은 것을 알 수 있습니다.<br/>
2. ReLU는 출력값이 우리가 원하는 0 ~ 1사이 값이 아니기 때문에, 맨 마지막 레이어는 Sigmoid 함수를 사용해야 합니다.
<br/>
<br/>

## 3. leakyReLU 함수, PReLU, ELU

### 1. ReLU의 단점을 보완하기 위한 leakyReLU, PReLU, ELU
1. ReLU함수 또한 단점이 있습니다. 모델이 학습하는 동안 일부 뉴런들이 0만 출력하여 활성화 되지 않는 문제인데 이러한 문제를 dead ReLU라고 합니다. 특히 학습률이 클 경우, 모델의 뉴런이 절반정도가 죽는 상황도 생깁니다. 
2. 뉴런이 0만 출력하는 이유는 학습이 진행되면서 뉴런의 가중치가 업데이트 되어 합이 음수가 되는 순간 ReLU에 의해 그 이후로는 0만 출력하게 되며, 이때 기울기 또한 0이 되어버리기 때문입니다. 이러한 Dead ReLU문제를 해결하기 위해 ReLU함수를 조금씩 변형시켜 다양한 ReLU를 만들어 사용하기도 합니다.

### 2. leakyReLU
$$
LeakyReLU_\alpha(x) = max(\alpha x, x)
$$
1. 위 식에서 $\alpha$는 0 이하인 입력에 대해 활성화 함수가 0만 출력하는 것이 아닌, $\alpha$만큼의 값을 곱해 출력하는 방법으로 dead ReLU문제를 해결합니다.
2. 일반적으로 $\alpha$는 0.01의 값으로 설정합니다.

### 3. PReLU
1. PReLU(parametric ReLU)는 Leaky ReLU와 식이 동일하지만, 하이퍼파라미터인 $\alpha$를 가중치 매개변수와 마찬가지로 역전파에 의해 학습되도록 하는 함수입니다.
2. 대구모 이미지 데이터셋에서는 ReLU보다 성능이 좋지만, 소규모 데이터셋에는 오버피팅 될 위험이 있습니다.

### 4. ELU
$$
ELU_\alpha = [\alpha(exp(x)-1), .......if:x<0], 
            [x, .........if:x>=0]   
$$

1. x < 0 일 때 ELU 활성화 함수 출력이 평균 0에 가까워지기 때문에 편향 이동(bias shift)이 감소하여 그래디언트 소실 문제를 줄여줍니다. 하이퍼파라미터인 \alpha는 x가 음수일 때 ELU가 수렴할 값을 정의하며 보통 1로 설정합니다.
2. x < 0 이어도 그래디언트가 0이 아니므로 Dead 뉴런을 만들지 않습니다.
3. \alpha = 1일 때 ELU는 x = 0에서 급격하게 변하지 않고 모든 구간에서 매끄럽게 변하기 때문에 경사하강법에서 수렴속도가 빠릅니다.


## 4. tanh 함수 (하이퍼볼릭 탄젠트)
$$ y = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
### 1. tanh 함수 특징 및 단점
1. tanh 함수는 sigmoid와 유사한 특징을 갖습니다. 고로 sigmoid형태로도 표현이 가능합니다. 
$ tanh(x) = 2sigmoid(2x) - 1$ 
2. sigmoid의 확장판이라고 불리는 만큼, sigmoid와 유사하게 tanh 함수는 범위를 -1 ~ 1사이로 갖습니다. sigmoid보다 출력범위가 큰 만큼 경사면이 더 크기 때문에, 더 빠르게 수렴하여 학습이 가능한 특성이 있습니다.
3. sigmoid와 유사한 만큼, sigmoid의 치명적인 단점인 vanishing gradient problem문제를 그대로 갖고 있습니다.
4. tanh 함수의 미분과정을 잘 설명해 놓은 블로그를 소개하겠습니다. [TAEWAN.KIM블로그](http://taewan.kim/post/tanh_diff/)

## 5. 어떤 활성화 함수를 써야할까?

1. 일반적으로는 ELU -> LeakyReLU -> ReLU -> tanh -> sigmoid 순으로 사용한다고 한다. cs231n 강의에서는 ReLU를 먼저 쓰고, 그 다음으로 LeakyReLU나 ELU 같은 ReLU Family를 사용하며, sigmoid는 사용하지 말라고 하고 있습니다.